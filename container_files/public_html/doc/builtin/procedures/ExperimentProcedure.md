# Classifier Experiment Procedure

The classifier experiment procedure is used to train and evaluate a classifier in a single run.
It wraps the ![](%%doclink classifier.train procedure) and the ![](%%doclink classifier.test procedure) into a single and easier to use procedure. It can also perform k-fold cross-validation by specifying multiple folds over the data to use for training and testing.

The `classifier.experiment` procedure will run multiple rounds of training and testing, based on the settings of the `inputData`, `kfold`, `datasetFolds` and `testingDataOverride` parameters, according to the [steps laid out below](#TrainTest).

## Configuration

![](%%config procedure classifier.experiment)

<a name="DatasetFoldConfig"></a>
## Dataset Folds and Cross-validation

The experiment procedure supports k-fold 
[cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)) in a
flexible way. Folds can be specified implicitly using the `kfolds` parameter, or passing in a DatasetFoldConfig to the `datasetFolds` parameter as follows:

### DatasetFoldConfig

![](%%type Datacratic::MLDB::DatasetFoldConfig)

### Example: 3-fold cross-validation

To perform a 3-fold cross-validation, you can set the parameter `kfold` to 3 or *equivalently*, set the `datasetFolds` parameter to the following value:

```python
[
    {
        "trainingWhere": "rowHash() % 3 != 0",
        "testingWhere": "rowHash() % 3 = 0",
    },
    {
        "trainingWhere": "rowHash() % 3 != 1",
        "testingWhere": "rowHash() % 3 = 1",
    },
    {
        "trainingWhere": "rowHash() % 3 != 2",
        "testingWhere": "rowHash() % 3 = 2",
    }
]
```

<a name="TrainTest"></a>
## Training and Testing Set Generation

The `classifier.experiment` procedure will run multiple rounds of training and testing, based on the settings of the `inputData`, `kfold`, `datasetFolds` and `testingDataOverride` parameters.

The procedure first assembles a DatasetFoldConfig using the following rules:

1. if `datasetFolds` is specified, that is the DatasetFoldConfig
1. else if `kfold` is specified, a DatasetFoldConfig is generated as per the example above
1. else if `testingDataOverride` is specified, the DatasetFoldConfig is `[{"trainingWhere": "true", "testingWhere": "true"}]` (i.e. the procedure will train on `inputData` and test on `testingDataOverride`)
1. else (i.e. **base case** of only `inputData` specified) the DatasetFoldConfig is `[{"trainingWhere": "rowHash() % 2 != 1", "testingWhere": "rowHash() % 2 = 1"}]` (i.e. the procedure will train on half of `inputData` and test on the other half)

The procedure then assembles training and testing sets for each entry in the DatasetFoldConfig:

1. The training set will be the result of the query built by combining `inputData` with the `trainingWhere`, `trainingOffset`, `trainingLimit` and `orderBy` parameters of the DatasetFoldConfig entry
1. The testing query will be the result of the query built by combining  `inputData` (or `testingDataOverride` if specified) with the `testingWhere`, `testingOffset`, `testingLimit` and `orderBy` parameters of the DatasetFoldConfig entry. The procedure will automatically use the `classifier` function generated by the training and call it with the features in the testing query to generate a score to compare to the label.


## Output

The output will contain performance metrics over each fold that was tested. See the 
![](%%doclink classifier.test procedure) page for a sample output.

An aggregated version of the metrics over all folds is also provided. The aggregated
results blob will have the same structure as each fold but every numeric metric will
be replaced by an object containing standard statistics (min, max, mean, std).
In the example below, the `auc` metric is used to illustrate the aggregation.

The following example would be for a 2-fold run:

```python
{
    "id" : "<id>",
    "runFinished" : "...",
    "runStarted" : "...",
    "state" : "finished",
    "status" : {
        "aggregatedTest": {
            "auc": {
                "max": x,
                "mean": y,
                "min": z,
                "std": k
            },
            ...
        },
        "folds": [
            {
                "accuracyDataset" : <id of fold 1 accuracy dataset if it was generated>,
                "modelFileUrl" = <path of fold 1 model file>,
                "functionName" = <name of fold 1 scorer function>,
                "resultsTest" : { <classifier.test output for fold 1> },
                "fold": { <datasetFold used for fold 1> }
            },
            {
                "accuracyDataset" : <id of fold 2 accuracy dataset if it was generated>,
                "modelFileUrl" = <path of fold 2 model file>,
                "functionName" = <name of fold 2 scorer function>,
                "resultsTest" : { <classifier.test output for fold 2> },
                "fold": { <datasetFold used for fold 2> }
            }
        ]
    }
}
```

If the training set is also evaluated, the additional keys `aggregatedTrain` and `resultsTrain` 
will also be returned and will have the same structure as the results for the testing set.

## Examples

* The ![](%%nblink _demos/Predicting Titanic Survival) demo notebook

## See also

* ![](%%doclink classifier.train procedure)
* ![](%%doclink classifier.test procedure)
* ![](%%doclink classifier function)
* [Cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)) on Wikipedia


